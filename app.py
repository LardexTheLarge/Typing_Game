import re
from flask import Flask, jsonify
from flask_cors import CORS
from llama_cpp import Llama
import itertools

app = Flask(__name__)
CORS(app)

llm_model = None
# list of fallback sentences
FALLBACK_SENTENCES = [
    "The quick brown fox jumps over the lazy dog.",
    "A journey of a thousand miles begins with a single step.",
    "The early bird catches the worm.",
    "Where there's a will, there's a way.",
    "All that glitters is not gold.",
    "It's always darkest before the dawn.",
    "Actions speak louder than words."
]
fallback_iterator = itertools.cycle(FALLBACK_SENTENCES)

def load_llm_model():
    """Loads the Llama model into the global llm_model variable."""
    global llm_model
    try:
        llm_model = Llama(
            model_path="./zephyr-quiklang-3b.Q4_K_M.gguf",
            n_ctx=16384,  # Max sequence length
            n_threads=8,  # Number of CPU threads
            n_gpu_layers=35  # Number of layers to offload to GPU (set to 0 if no GPU)
        )
        print("Llama model loaded successfully.")
    except Exception as e:
        print(f"Error loading Llama model: {e}")
        llm_model = None

def clean_generated_sentence(raw_text: str) -> str:
    """Cleans the raw output from the LLM to get a usable sentence."""
    if not raw_text:
        return ""

    # Remove common instructional prefixes or example markers
    text = re.sub(r"^(example\s*\d*[:\-]*\s*|sentence\s*[:\-]*\s*|response\s*[:\-]*\s*|\d+\.\s*)", "", raw_text.strip(), flags=re.IGNORECASE)
    
    # Strip leading/trailing quotes and whitespace
    text = text.strip().strip('"\'')
    
    # Take the first actual line if multiple are generated by mistake
    text = text.split('\n')[0].strip()

    # Ensure the sentence is not just whitespace and has some alphanumeric characters
    if not re.search(r"[a-zA-Z0-9]", text):
        return ""

    # Capitalize the first letter
    if text:
        text = text[0].upper() + text[1:]

    # Ensure it ends with a standard punctuation mark if it looks like a sentence.
    # If it's very short or doesn't seem to have content, don't append.
    if text and len(text) > 5 and not re.search(r"[.!?]$", text):
        text += "."
        
    return text

def generate_single_sentence(prompt: str, max_retries=3):
    """Generates a single clean sentence using the LLM."""
    if llm_model is None:
        return "Error: Model not loaded."

    for _ in range(max_retries):
        try:
            output = llm_model(
                prompt,
                max_tokens=20,
                stop=["\n", ".", "!", "?"],
                echo=False
            )
            
            if output and output["choices"] and len(output["choices"]) > 0:
                raw_sentence = output["choices"][0]["text"]
                cleaned_sentence = clean_generated_sentence(raw_sentence)
                if cleaned_sentence and len(cleaned_sentence.split()) > 4:
                    return cleaned_sentence
        except Exception as e:
            print(f"Error during sentence generation: {e}")
            continue # Retry
    return next(fallback_iterator) # Fallback after retries

@app.route('/get-sentences', methods=['GET'])
def get_sentences_api():
    if llm_model is None:
        return jsonify({"error": "Model not loaded. Please check server logs."}), 500

    num_sentences = 10
    sentences_list = []
    prompt = "Write a sentence designed to test a userâ€™s typing speed and accuracy."
    print(f"Generating {num_sentences} sentences...")
    for i in range(num_sentences):
        sentence = generate_single_sentence(prompt)
        # Ensure variety or handle potential duplicates if necessary, assume variety from LLM
        print(f"Generated sentence {i+1}: {sentence}")
        sentences_list.append(sentence)
    
    return jsonify(sentences_list)

if __name__ == '__main__':
    load_llm_model() # Load the model when the script starts
    if llm_model:
        app.run(debug=True, host='0.0.0.0', port=5000)
    else:
        print("Failed to load model. Flask app will not start.")